{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCAt3pdCBXh/B3bmXKXDOq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshilade/EDS-video/blob/main/Sakshi_Lade_DeepLearningLabAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NAME:SAKSHI LADE\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ROLL NO.: 74\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "BATCH: A4\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "DATE OF SUBMISSION: 25/01/25\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QDGid4dQ7isA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part A: Helper Functions\n",
        "\n",
        "import random\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return sum((yt - yp) ** 2 for yt, yp in zip(y_true, y_pred)) / len(y_true)\n",
        "\n",
        "# Part B: Feedforward Neural Network Class\n",
        "\n",
        "class FeedforwardNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        \"\"\"Initialize the neural network with random weights and biases.\"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases with random values\n",
        "        self.weights_input_hidden = [[random.uniform(-1, 1) for _ in range(hidden_size)] for _ in range(input_size)]\n",
        "        self.bias_hidden = [0 for _ in range(hidden_size)]\n",
        "        self.weights_hidden_output = [[random.uniform(-1, 1) for _ in range(output_size)] for _ in range(hidden_size)]\n",
        "        self.bias_output = [0 for _ in range(output_size)]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Perform a forward pass through the network.\"\"\"\n",
        "        # Input to hidden layer\n",
        "        self.hidden_input = [\n",
        "            sum(x * w + b for x, w, b in zip(X, weights, [self.bias_hidden[j]] * len(X)))\n",
        "            for j, weights in enumerate(zip(*self.weights_input_hidden))\n",
        "        ]\n",
        "        self.hidden_output = [sigmoid(h) for h in self.hidden_input]\n",
        "\n",
        "        # Hidden to output layer\n",
        "        self.output_input = [\n",
        "            sum(h * w + b for h, w, b in zip(self.hidden_output, weights, [self.bias_output[k]] * len(self.hidden_output)))\n",
        "            for k, weights in enumerate(zip(*self.weights_hidden_output))\n",
        "        ]\n",
        "        self.output_output = [sigmoid(o) for o in self.output_input]\n",
        "\n",
        "        return self.output_output\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        \"\"\"Perform backward propagation and update weights and biases.\"\"\"\n",
        "        # Calculate output layer error\n",
        "        error_output = [yt - yp for yt, yp in zip(y, output)]\n",
        "        delta_output = [eo * sigmoid_derivative(oo) for eo, oo in zip(error_output, output)]\n",
        "\n",
        "        # Calculate hidden layer error\n",
        "        error_hidden = [\n",
        "            sum(delta_output[k] * self.weights_hidden_output[j][k] for k in range(self.output_size))\n",
        "            for j in range(self.hidden_size)\n",
        "        ]\n",
        "        delta_hidden = [eh * sigmoid_derivative(ho) for eh, ho in zip(error_hidden, self.hidden_output)]\n",
        "\n",
        "        # Update weights and biases\n",
        "        for j in range(self.hidden_size):\n",
        "            for i in range(self.input_size):\n",
        "                self.weights_input_hidden[i][j] += self.learning_rate * delta_hidden[j] * X[i]\n",
        "            self.bias_hidden[j] += self.learning_rate * delta_hidden[j]\n",
        "\n",
        "        for k in range(self.output_size):\n",
        "            for j in range(self.hidden_size):\n",
        "                self.weights_hidden_output[j][k] += self.learning_rate * delta_output[k] * self.hidden_output[j]\n",
        "            self.bias_output[k] += self.learning_rate * delta_output[k]\n",
        "\n",
        "    def train(self, X, y, epochs):\n",
        "        \"\"\"Train the neural network over a number of epochs.\"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for xi, yi in zip(X, y):\n",
        "                output = self.forward(xi)\n",
        "                self.backward(xi, yi, output)\n",
        "                epoch_loss += mse(yi, output)\n",
        "\n",
        "            # Print loss every 100 epochs\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {epoch_loss / len(X):.4f}\")\n",
        "\n",
        "# Part C: Example Usage\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Input data (4 samples, 2 features each)\n",
        "    X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "\n",
        "    # Output labels (4 samples, 1 output each)\n",
        "    y = [[0], [1], [1], [0]]\n",
        "\n",
        "    # Create the neural network\n",
        "    nn = FeedforwardNeuralNetwork(input_size=2, hidden_size=2, output_size=1, learning_rate=0.1)\n",
        "\n",
        "    # Train the network\n",
        "    nn.train(X, y, epochs=10000)\n",
        "\n",
        "    # Test the network\n",
        "    print(\"Trained outputs:\")\n",
        "    for xi in X:\n",
        "        print(nn.forward(xi))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg2dtpDt321D",
        "outputId": "64f5f734-6b78-49ee-fbe9-d447fe31955e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2545\n",
            "Epoch 100, Loss: 0.2544\n",
            "Epoch 200, Loss: 0.2543\n",
            "Epoch 300, Loss: 0.2543\n",
            "Epoch 400, Loss: 0.2542\n",
            "Epoch 500, Loss: 0.2542\n",
            "Epoch 600, Loss: 0.2541\n",
            "Epoch 700, Loss: 0.2540\n",
            "Epoch 800, Loss: 0.2539\n",
            "Epoch 900, Loss: 0.2538\n",
            "Epoch 1000, Loss: 0.2536\n",
            "Epoch 1100, Loss: 0.2534\n",
            "Epoch 1200, Loss: 0.2531\n",
            "Epoch 1300, Loss: 0.2527\n",
            "Epoch 1400, Loss: 0.2522\n",
            "Epoch 1500, Loss: 0.2516\n",
            "Epoch 1600, Loss: 0.2507\n",
            "Epoch 1700, Loss: 0.2497\n",
            "Epoch 1800, Loss: 0.2483\n",
            "Epoch 1900, Loss: 0.2466\n",
            "Epoch 2000, Loss: 0.2445\n",
            "Epoch 2100, Loss: 0.2419\n",
            "Epoch 2200, Loss: 0.2387\n",
            "Epoch 2300, Loss: 0.2348\n",
            "Epoch 2400, Loss: 0.2303\n",
            "Epoch 2500, Loss: 0.2251\n",
            "Epoch 2600, Loss: 0.2195\n",
            "Epoch 2700, Loss: 0.2135\n",
            "Epoch 2800, Loss: 0.2074\n",
            "Epoch 2900, Loss: 0.2011\n",
            "Epoch 3000, Loss: 0.1948\n",
            "Epoch 3100, Loss: 0.1883\n",
            "Epoch 3200, Loss: 0.1812\n",
            "Epoch 3300, Loss: 0.1730\n",
            "Epoch 3400, Loss: 0.1632\n",
            "Epoch 3500, Loss: 0.1512\n",
            "Epoch 3600, Loss: 0.1368\n",
            "Epoch 3700, Loss: 0.1206\n",
            "Epoch 3800, Loss: 0.1036\n",
            "Epoch 3900, Loss: 0.0874\n",
            "Epoch 4000, Loss: 0.0730\n",
            "Epoch 4100, Loss: 0.0610\n",
            "Epoch 4200, Loss: 0.0512\n",
            "Epoch 4300, Loss: 0.0434\n",
            "Epoch 4400, Loss: 0.0372\n",
            "Epoch 4500, Loss: 0.0322\n",
            "Epoch 4600, Loss: 0.0282\n",
            "Epoch 4700, Loss: 0.0250\n",
            "Epoch 4800, Loss: 0.0223\n",
            "Epoch 4900, Loss: 0.0201\n",
            "Epoch 5000, Loss: 0.0182\n",
            "Epoch 5100, Loss: 0.0166\n",
            "Epoch 5200, Loss: 0.0152\n",
            "Epoch 5300, Loss: 0.0140\n",
            "Epoch 5400, Loss: 0.0130\n",
            "Epoch 5500, Loss: 0.0121\n",
            "Epoch 5600, Loss: 0.0113\n",
            "Epoch 5700, Loss: 0.0106\n",
            "Epoch 5800, Loss: 0.0100\n",
            "Epoch 5900, Loss: 0.0094\n",
            "Epoch 6000, Loss: 0.0089\n",
            "Epoch 6100, Loss: 0.0084\n",
            "Epoch 6200, Loss: 0.0080\n",
            "Epoch 6300, Loss: 0.0076\n",
            "Epoch 6400, Loss: 0.0073\n",
            "Epoch 6500, Loss: 0.0070\n",
            "Epoch 6600, Loss: 0.0067\n",
            "Epoch 6700, Loss: 0.0064\n",
            "Epoch 6800, Loss: 0.0061\n",
            "Epoch 6900, Loss: 0.0059\n",
            "Epoch 7000, Loss: 0.0057\n",
            "Epoch 7100, Loss: 0.0055\n",
            "Epoch 7200, Loss: 0.0053\n",
            "Epoch 7300, Loss: 0.0051\n",
            "Epoch 7400, Loss: 0.0049\n",
            "Epoch 7500, Loss: 0.0048\n",
            "Epoch 7600, Loss: 0.0046\n",
            "Epoch 7700, Loss: 0.0045\n",
            "Epoch 7800, Loss: 0.0043\n",
            "Epoch 7900, Loss: 0.0042\n",
            "Epoch 8000, Loss: 0.0041\n",
            "Epoch 8100, Loss: 0.0040\n",
            "Epoch 8200, Loss: 0.0039\n",
            "Epoch 8300, Loss: 0.0038\n",
            "Epoch 8400, Loss: 0.0037\n",
            "Epoch 8500, Loss: 0.0036\n",
            "Epoch 8600, Loss: 0.0035\n",
            "Epoch 8700, Loss: 0.0034\n",
            "Epoch 8800, Loss: 0.0033\n",
            "Epoch 8900, Loss: 0.0033\n",
            "Epoch 9000, Loss: 0.0032\n",
            "Epoch 9100, Loss: 0.0031\n",
            "Epoch 9200, Loss: 0.0030\n",
            "Epoch 9300, Loss: 0.0030\n",
            "Epoch 9400, Loss: 0.0029\n",
            "Epoch 9500, Loss: 0.0029\n",
            "Epoch 9600, Loss: 0.0028\n",
            "Epoch 9700, Loss: 0.0027\n",
            "Epoch 9800, Loss: 0.0027\n",
            "Epoch 9900, Loss: 0.0026\n",
            "Trained outputs:\n",
            "[0.05197660651452505]\n",
            "[0.9503400508618015]\n",
            "[0.9501141099913473]\n",
            "[0.05204344596183549]\n"
          ]
        }
      ]
    }
  ]
}